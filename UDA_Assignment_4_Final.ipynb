{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDA Assignment 4: REI Instagram Engagement Analysis\n",
    "\n",
    "Created by: Aman Bhardwaj, Blake DeLong, Apurva Harsulkar, Colby Meline, Joel Nail, and Rahul Rangarao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "Rather than analyze National Geographic's Instagram page, we chose to investigate REI, also known as Recreational Equipment, Inc. REI mainly sells camping and hiking gear, and they are known for being an \"outdoorsy\" brand that encourages its customers to be physically active and enjoy the beauty of nature. Compared to National Geographic's Instagram page, REI's account features more posts related to hiking, camping, and other outdoor activities. While REI features fewer animals that Nat Geo, they share a common appreciation for nature and the beautiful landscapes it has to offer. We believe that by analyzing REI's posts, we can offer valuable insights that will help the brand increase its Instagram engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraper\n",
    "\n",
    "We initially created a scraper using Selenium to extract posts from REI's instagram page. We later discovered a service called PhantomBuster which allowed us to easily scrape everything we needed from REI's account. We have included the Selenium code below as reference, but the data we analyzed was obtained from PhantomBuster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "# Setup Firefox\n",
    "service = Service(executable_path=\"/home/colby/selenium/geckodriver\")\n",
    "firefox_options = Options()\n",
    "firefox_options.binary = FirefoxBinary(\"/bin/firefox\")\n",
    "firefox_options.headless = False\n",
    "driver = webdriver.Firefox(options=firefox_options, service=service)\n",
    "a = ActionChains(driver)\n",
    "\n",
    "\n",
    "insta_page = \"https://www.instagram.com/rei/\"\n",
    "driver.get(insta_page)\n",
    "\n",
    "\n",
    "def driver_login():\n",
    "    username = driver.find_element(by=By.NAME, value=\"username\")\n",
    "    password = driver.find_element(by=By.NAME, value=\"password\")\n",
    "    submit_button = driver.find_element(by=By.CSS_SELECTOR, value=\"button\")\n",
    "\n",
    "    username.clear()\n",
    "    password.clear()\n",
    "    username.send_keys(\"msitm_student\")\n",
    "    time.sleep(2)\n",
    "    password.send_keys(\"UDA4Project\")\n",
    "    submit_button.click()\n",
    "    time.sleep(10)\n",
    "\n",
    "    try:\n",
    "        save_info_not_now = driver.find_element(by=By.XPATH, value=\"//button[contains(text(), 'Not Now')]\").click()\n",
    "        time.sleep(10)\n",
    "    except NoSuchElementException:\n",
    "        print(\"No save info redirect\")\n",
    "        \n",
    "    try:\n",
    "        notif_not_now = driver.find_element(by=By.XPATH, value=\"//button[contains(text(), 'Not Now')]\").click()\n",
    "        time.sleep(10)\n",
    "    except NoSuchElementException:\n",
    "        print(\"No notify redirect\")\n",
    "        \n",
    "    driver.get(\"https://www.instagram.com/rei\")\n",
    "    \n",
    "time.sleep(4)\n",
    "driver_login()\n",
    "\n",
    "time.sleep(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = driver.find_elements(by=By.TAG_NAME, value='img')\n",
    "boxes = driver.find_elements(by=By.TAG_NAME, value='a')\n",
    "\n",
    "df = {\"image\": [], \"likes\": [], \"comments\": []}\n",
    "\n",
    "if False:\n",
    "    for i, image, box in enumerate(zip(images, boxes)):\n",
    "        print(\".\", sep=\"\")\n",
    "        time.sleep(2)\n",
    "        images[1].location_once_scrolled_into_view\n",
    "        image.location_once_scrolled_into_view\n",
    "        with open(\"rei/rei{}.png\".format(i), \"wb\") as file:\n",
    "            file.write(images[i].screenshot_as_png)    \n",
    "        a.move_to_element(image).perform()\n",
    "        time.sleep(1)\n",
    "    #     parent = images[i]\n",
    "    #     while parent.tag_name != \"a\":\n",
    "    #         parent = parent.find_element(By.XPATH, \"..\")\n",
    "    #     spans = parent.find_elements(By.TAG_NAME, \"span\")\n",
    "\n",
    "        spans = box.find_elements(By.TAG_NAME, \"span\")\n",
    "        if len(spans) == 4:\n",
    "            df[\"image\"].append(image.get_attribute(\"src\"))\n",
    "            likes, comments = [i.text for i in spans if i.text]\n",
    "            df[\"likes\"].append(likes)\n",
    "            df[\"comments\"].append(comments)\n",
    "    \n",
    "\n",
    "boxes = [i for i in boxes if \"/p/\" in i.get_attribute(\"href\")]\n",
    "previous = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate():\n",
    "    global boxes\n",
    "    global previous\n",
    "    for box in boxes:\n",
    "        if box not in previous:\n",
    "            time.sleep(1)\n",
    "            previous.add(box)\n",
    "            box.location_once_scrolled_into_view\n",
    "            try:\n",
    "                image = box.find_element(By.TAG_NAME, \"img\")\n",
    "                with open(\"rei/rei{}.png\".format(box.id), \"wb\") as file:\n",
    "                    file.write(image.screenshot_as_png)  \n",
    "                a.move_to_element(image).perform()\n",
    "                spans = box.find_elements(By.TAG_NAME, \"span\")\n",
    "                if len(spans) == 4:\n",
    "                    df[\"image\"].append(image.get_attribute(\"src\"))\n",
    "                    likes, comments = [i.text for i in spans if i.text]\n",
    "                    df[\"likes\"].append(likes)\n",
    "                    df[\"comments\"].append(comments)\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "    return previous\n",
    "            \n",
    "while len(df[\"image\"]) < 300:\n",
    "    iterate()\n",
    "    footer = driver.find_element(by=By.TAG_NAME, value=\"footer\")\n",
    "    footer.location_once_scrolled_into_view\n",
    "    time.sleep(10)\n",
    "    boxes = driver.find_elements(by=By.TAG_NAME, value='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataframe = pd.DataFrame(df)\n",
    "dataframe.to_csv(\"sample_rei.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Vision\n",
    "\n",
    "In order to determine what objects are present in the scraped images, we utilized Google Cloud's Vision API. The code below was used to obtain labels for each REI post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from google.cloud import vision\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"\"  ###The Google Cloud vision creds\n",
    "df=pd.read_csv(\"result.csv\") ##Scraped from Instagram\n",
    "df=df[df['type']=='Photo'].reset_index().drop('index',axis=1)[['postUrl','description','imgUrl','likeCount','commentCount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_labels_uri(uri):\n",
    "    client = vision.ImageAnnotatorClient()\n",
    "    image = vision.Image()\n",
    "    image.source.image_uri = uri\n",
    "    response = client.label_detection(image=image)\n",
    "    labels = response.label_annotations\n",
    "    res=''\n",
    "    for i in labels:\n",
    "        res=res+' '+i.description\n",
    "    return res.lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['image_text']=df['imgUrl'].apply(lambda x:detect_labels_uri(x))\n",
    "grouped_df=df.groupby(['postUrl'])['image_text'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "grouped_df_1=df.groupby(['postUrl']).agg({'description':'max','likeCount':'max','commentCount':'max'}).reset_index()\n",
    "final_df=pd.merge(grouped_df,grouped_df_1,left_on='postUrl',right_on='postUrl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engagement Metric\n",
    "\n",
    "In order to properly evaluate the engagement of REI's Instagram posts, we first normalized the number of likes and comments on each post by dividing by the maximum number of likes and comments respectively. We then used a weighted average of these normalized scores to develop an overall engagement metric for each post. To allow for binary logistic regression, we labelled posts with an engagement metric higher than the median score as High Engagement (1) and posts with an engagement metric lower than the median score as Low Engagement (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rei_lda_df = pd.read_csv(\"REI_LDA_topic.csv\")\n",
    "rei_lda_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "rei_norm_df = rei_lda_df.copy()\n",
    "\n",
    "commentCountMax = max(rei_norm_df['commentCount'])\n",
    "likeCountMax = max(rei_norm_df['likeCount'])\n",
    "\n",
    "rei_norm_df['commentCountNorm'] = rei_norm_df['commentCount'] / commentCountMax\n",
    "rei_norm_df['likeCountNorm'] = rei_norm_df['likeCount'] / likeCountMax\n",
    "\n",
    "# post 13 has the highest comment count\n",
    "# post 38 has the highest like count\n",
    "\n",
    "# since likes are cheap and comments can be hard to come by, we have weighted our comment count higher than our like count\n",
    "# this assumes that REI prioritizes comments over likes when it comes to engagement\n",
    "rei_norm_df['engagementMetric'] = (rei_norm_df['commentCountNorm'] * .6 + rei_norm_df['likeCountNorm'] * .4)\n",
    "\n",
    "#print(rei_norm_df['engagementMetric'].idxmax())\n",
    "\n",
    "median_engagement_score = np.median(rei_norm_df['engagementMetric'])\n",
    "print(median_engagement_score)\n",
    "\n",
    "rei_norm_df['engagementBinary'] = np.where(rei_norm_df['engagementMetric'] < median_engagement_score, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "We utilized tf-idf scores in order to perform word replacement on our image labels and post captions. Following this step, we performed three different logistic regressions to predict engagement. The first regression used only image labels, the second regression utilized the post captions, and the final regression used both image labels and post captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('REI_LDA_topic.csv').drop([185]).reset_index()\n",
    "\n",
    "string_punctuation = '''()-[]{};:'\"\\,<>./?@#$%^&*_~1234567890'''\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    no_punct = \"\"\n",
    "    for letter in s:\n",
    "        if letter not in string_punctuation:\n",
    "            no_punct += letter\n",
    "    return no_punct\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF for Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[ :, 3] = df.iloc[ :, 3].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for row in df['description']:\n",
    "    df.iloc[ i, 3] = remove_punctuation(row)\n",
    "    i=i+1\n",
    "df['description'] = df['description'].str.replace(\"!\",\" !\")\n",
    "df['description'] = df['description'].apply(word_tokenize)\n",
    "df['description'] = df['description'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "df['description'] = df['description'].apply(lambda x : \" \".join(x))\n",
    "\n",
    "Text_Column = df.iloc[ :, 3]\n",
    "\n",
    "sklearn_tfidf = TfidfVectorizer(min_df=.01, max_df =.95, stop_words=\"english\",use_idf=True, smooth_idf=False, sublinear_tf=True)\n",
    "sklearn_representation = sklearn_tfidf.fit_transform(Text_Column.tolist())\n",
    "Tfidf_Output_caption = pd.DataFrame(sklearn_representation.toarray(), columns=sklearn_tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF for Image Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('REI_LDA_topic.csv').drop([185]).reset_index()\n",
    "\n",
    "string_punctuation = '''()-[]{};:'\"\\,<>./?@#$%^&*_~1234567890'''\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[ :, 2] = df.iloc[ :, 2].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for row in df['image_text']:\n",
    "    df.iloc[ i, 2] = remove_punctuation(row)\n",
    "    i=i+1\n",
    "df['image_text'] = df['image_text'].str.replace(\"!\",\" !\")\n",
    "df['image_text'] = df['image_text'].apply(word_tokenize)\n",
    "df['image_text'] = df['image_text'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "df['image_text'] = df['image_text'].apply(lambda x : \" \".join(x))\n",
    "\n",
    "Text_Column = df.iloc[ :, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_tfidf = TfidfVectorizer(min_df=.01, max_df =.95, stop_words=\"english\",use_idf=True, smooth_idf=False, sublinear_tf=True)\n",
    "sklearn_representation = sklearn_tfidf.fit_transform(Text_Column.tolist())\n",
    "Tfidf_Output_image = pd.DataFrame(sklearn_representation.toarray(), columns=sklearn_tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to Display Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(C,class_labels=['0','1']):\n",
    "\n",
    "    assert C.shape == (2,2), \"Confusion matrix should be from binary classification only.\"\n",
    "\n",
    "    # true negative, false positive, etc...\n",
    "    tn = C[0,0]; fp = C[0,1]; fn = C[1,0]; tp = C[1,1];\n",
    "\n",
    "    NP = fn+tp # Num positive examples\n",
    "    NN = tn+fp # Num negative examples\n",
    "    N  = NP+NN\n",
    "\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax  = fig.add_subplot(111)\n",
    "    ax.imshow(C, interpolation='nearest', cmap=plt.cm.gray)\n",
    "\n",
    "    # Draw the grid boxes\n",
    "    ax.set_xlim(-0.5,2.5)\n",
    "    ax.set_ylim(2.5,-0.5)\n",
    "    ax.plot([-0.5,2.5],[0.5,0.5], '-k', lw=2)\n",
    "    ax.plot([-0.5,2.5],[1.5,1.5], '-k', lw=2)\n",
    "    ax.plot([0.5,0.5],[-0.5,2.5], '-k', lw=2)\n",
    "    ax.plot([1.5,1.5],[-0.5,2.5], '-k', lw=2)\n",
    "\n",
    "    # Set xlabels\n",
    "    ax.set_xlabel('Predicted Label', fontsize=16)\n",
    "    ax.set_xticks([0,1,2])\n",
    "    ax.set_xticklabels(class_labels + [''])\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    ax.xaxis.tick_top()\n",
    "    # These coordinate might require some tinkering. Ditto for y, below.\n",
    "    ax.xaxis.set_label_coords(0.34,1.06)\n",
    "\n",
    "    # Set ylabels\n",
    "    ax.set_ylabel('True Label', fontsize=16, rotation=90)\n",
    "    ax.set_yticklabels(class_labels + [''],rotation=90)\n",
    "    ax.set_yticks([0,1,2])\n",
    "    ax.yaxis.set_label_coords(-0.09,0.65)\n",
    "\n",
    "\n",
    "    # Fill in initial metrics: tp, tn, etc...\n",
    "    ax.text(0,0,\n",
    "            '%d'%(tn),\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(0,1,\n",
    "            '%d'%fn,\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(1,0,\n",
    "            '%d'%fp,\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "\n",
    "    ax.text(1,1,\n",
    "            '%d'%(tp),\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "    # Fill in secondary metrics: accuracy, true pos rate, etc...\n",
    "    ax.text(2,0,\n",
    "            'Error: %.2f'%(fp / (fp+tn+0.)),\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(2,1,\n",
    "            'Error: %.2f'%(fn / (tp+fn+0.)),\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(2,2,\n",
    "            'Accuracy: %.2f'%((tp+tn+0.)/N),\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(0,2,' ',\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(1,2,\n",
    "            ' ',\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w',boxstyle='round,pad=1'))\n",
    "\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rei_df = pd.read_csv(\"REI_LDA_topic.csv\").drop([185]).reset_index()\n",
    "rei_norm_df = pd.read_csv(\"REI_LDA_topic.csv\").drop([185]).reset_index()\n",
    "\n",
    "commentCountMax = max(rei_df['commentCount'])\n",
    "likeCountMax = max(rei_df['likeCount'])\n",
    "\n",
    "rei_norm_df['commentCountNorm'] = rei_norm_df['commentCount'] / commentCountMax\n",
    "rei_norm_df['likeCountNorm'] = rei_norm_df['likeCount'] / likeCountMax\n",
    "\n",
    "# post 13 has the highest comment count\n",
    "# post 38 has the highest like count\n",
    "\n",
    "# since likes are cheap and comments can be hard to come by, we have weighted our comment count higher than our like count\n",
    "# this assumes that REI prioritizes comments over likes when it comes to engagement\n",
    "rei_norm_df['engagementMetric'] = (rei_norm_df['commentCountNorm'] * .6 + rei_norm_df['likeCountNorm'] * .4)\n",
    "\n",
    "#print(rei_norm_df['engagementMetric'].idxmax())\n",
    "\n",
    "median_engagement_score = np.median(rei_norm_df['engagementMetric'])\n",
    "print(median_engagement_score)\n",
    "\n",
    "rei_norm_df['engagementBinary'] = np.where(rei_norm_df['engagementMetric'] < median_engagement_score, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression with Post Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = pd.concat([rei_norm_df, Tfidf_Output_caption], axis=1)\n",
    "\n",
    "y = Input['engagementBinary'].to_list()\n",
    "X = Input.loc[:, Tfidf_Output_caption.columns]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y ,random_state=42, test_size=0.3)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.tolist()\n",
    "\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "show_confusion_matrix(matrix, ['0', '1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](caption_conf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression with Image Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = pd.concat([rei_norm_df, Tfidf_Output_image], axis=1)\n",
    "\n",
    "y = Input['engagementBinary'].to_list()\n",
    "X = Input.loc[:, Tfidf_Output_image.columns]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y ,random_state=42, test_size=0.3)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.tolist()\n",
    "\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "show_confusion_matrix(matrix, ['0', '1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](image_label_conf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression with Image Labels and Post Captions Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = pd.concat([rei_norm_df, Tfidf_Output_image, Tfidf_Output_caption], axis=1)\n",
    "\n",
    "columns = Tfidf_Output_image.columns\n",
    "columns = columns.append(Tfidf_Output_caption.columns)\n",
    "\n",
    "y = Input['engagementBinary'].to_list()\n",
    "X = Input.loc[:, columns]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y ,random_state=42, test_size=0.3)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.tolist()\n",
    "\n",
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "show_confusion_matrix(matrix, ['0', '1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](combined_conf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, using post captions was the best method for predicting the engagement of REI's Instagram posts although all three method produced fairly similar results. We believe this is because REI has informative post captions that are indicative of the post's content - something that is not true for all Instagram accounts. It is important to remember that these findings are specific to REI's account and may not apply to other brands' Instagram pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling\n",
    "\n",
    "We performed topic modeling on both the image labels and the post captions we obtained. We began with five topics but we found too much overlap between the topics at this level. After reducing the number of topics to four, we were happy with the categories that the algorithm produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from nltk.tokenize import PunktSentenceTokenizer, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import lda\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "word_tokenizer=RegexpTokenizer(r'\\w+')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stopwords_nltk=set(stopwords.words('english'))\n",
    "\n",
    "def tokenize_text(version_desc):\n",
    "    lowercase=version_desc.lower()\n",
    "    text = wordnet_lemmatizer.lemmatize(lowercase)\n",
    "    tokens = word_tokenizer.tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "vec_words = CountVectorizer(tokenizer=tokenize_text,stop_words=stopwords_nltk,decode_error='ignore')\n",
    "total_features_words = vec_words.fit_transform(final_df['image_text'])\n",
    "model = lda.LDA(n_topics=int(4), n_iter=500, random_state=1,eta=0.05)\n",
    "model.fit(total_features_words)\n",
    "topic_word = model.topic_word_\n",
    "doc_topic=model.doc_topic_\n",
    "doc_topic=pd.DataFrame(doc_topic)\n",
    "final_df=final_df.join(doc_topic)\n",
    "res=pd.DataFrame()\n",
    "\n",
    "for i in range(int(4)):\n",
    "    topic=\"topic_\"+str(i)\n",
    "    res[topic]=final_df.groupby(['postUrl'])[i].mean()\n",
    "res=res.reset_index()\n",
    "topics=pd.DataFrame(topic_word)\n",
    "topics.columns=vec_words.get_feature_names()\n",
    "topics1=topics.transpose()\n",
    "\n",
    "\n",
    "topics1.to_csv(\"topics_img.csv\")\n",
    "\n",
    "final_df=final_df.rename({0:'img_text_promos',1:'img_landscape',2:'img_campers/caravans',3:'img_outdoor_acitivies'},axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modeling for Post Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_features_words = vec_words.fit_transform(final_df['description'])\n",
    "model = lda.LDA(n_topics=int(4), n_iter=500, random_state=1,eta=0.05)\n",
    "model.fit(total_features_words)\n",
    "topic_word = model.topic_word_\n",
    "doc_topic=model.doc_topic_\n",
    "doc_topic=pd.DataFrame(doc_topic)\n",
    "final_df=final_df.join(doc_topic)\n",
    "res=pd.DataFrame()\n",
    "\n",
    "for i in range(int(4)):\n",
    "    topic=\"topic_\"+str(i)\n",
    "    res[topic]=final_df.groupby(['postUrl'])[i].mean()\n",
    "res=res.reset_index()\n",
    "topics=pd.DataFrame(topic_word)\n",
    "topics.columns=vec_words.get_feature_names()\n",
    "topics1=topics.transpose()\n",
    "\n",
    "\n",
    "topics1.to_csv(\"topics_caption.csv\")\n",
    "\n",
    "final_df=final_df.rename({0:'cap_rei_challenges',1:'cap_outdoor_running',2:'cap_hiking_gear',3:'cap_rei_promos'},axis=1)\n",
    "\n",
    "final_df=final_df.rename({'landscape':'img_landscape','campers/caravans':'img_campers_caravans'\n",
    "            ,'outdoor_activities':'img_outdoor_acitivites'},axis=1)\n",
    "\n",
    "final_df.to_csv(\"REI_LDA_topic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quartile Analysis\n",
    "\n",
    "Utilizing the topics generated above, we split our dataset into quarters based on the previously created engagement metric. We then compared the top quartile's average topic weights with the bottom quartile's average topic weights in order to determine if certain topics are more likely to encourage high engagement. You can see our findings below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_engagement = rei_norm_df['engagementMetric']\n",
    "\n",
    "engagement_top_quartile = np.quantile(just_engagement, .75)\n",
    "engagement_bottom_quartile = np.quantile(just_engagement, .25)\n",
    "\n",
    "\n",
    "# code to get the index values for the top quartile\n",
    "top_quart_idx = np.where(rei_norm_df['engagementMetric'] >= engagement_top_quartile, rei_norm_df.index, None)\n",
    "top_quart_idx_list = np.ndarray.tolist(top_quart_idx)\n",
    "for row in top_quart_idx:\n",
    "    try:\n",
    "        top_quart_idx_list.remove(None)\n",
    "    except ValueError:\n",
    "        pass\n",
    "# print(len(top_quart_idx_list)) # should be 92 (1/4 of our dataset)\n",
    "\n",
    "\n",
    "# code to get the index values for the bottom quartile\n",
    "bottom_quart_idx = np.where(rei_norm_df['engagementMetric'] <= engagement_bottom_quartile, rei_norm_df.index, None)\n",
    "bottom_quart_idx_list = np.ndarray.tolist(bottom_quart_idx)\n",
    "for row in bottom_quart_idx:\n",
    "    try:\n",
    "        bottom_quart_idx_list.remove(None)\n",
    "    except ValueError:\n",
    "        pass\n",
    "# print(len(bottom_quart_idx_list)) # should be 92 (1/4 of our dataset)\n",
    "\n",
    "\n",
    "# now let's get dataframes for the bottom and top quartiles\n",
    "top_quart_df = rei_norm_df[rei_norm_df.index.isin(top_quart_idx_list)]\n",
    "bottom_quart_df = rei_norm_df[rei_norm_df.index.isin(bottom_quart_idx_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_analyze = ['img_text_promos', 'img_landscape', 'img_campers_caravans', 'outdoor_acitivies', 'cap_rei_challenges',\n",
    " 'cap_outdoor_running', 'cap_hiking_gear', 'cap_rei_promos']\n",
    "\n",
    "print(\"Top Quartile\")\n",
    "for col in cols_to_analyze:\n",
    "    print(col + \":  \\t\" + str(np.mean(top_quart_df[col])))\n",
    "\n",
    "print(\"\\nBottom Quartile\")\n",
    "for col in cols_to_analyze:\n",
    "    print(col + \":  \\t\" + str(np.mean(bottom_quart_df[col])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_analyze = ['img_text_promos', 'img_landscape', 'img_campers_caravans', 'outdoor_acitivies', 'cap_rei_challenges',\n",
    " 'cap_outdoor_running', 'cap_hiking_gear', 'cap_rei_promos']\n",
    "\n",
    "quartile_df = pd.DataFrame(columns=[\"Top_Quartile\", \"Bottom_Quartile\", \"Difference\"], index=cols_to_analyze)\n",
    "\n",
    "for col in cols_to_analyze:\n",
    "    quartile_df.loc[col].loc['Top_Quartile'] = np.mean(top_quart_df[col])\n",
    "\n",
    "for col in cols_to_analyze:\n",
    "    quartile_df.loc[col].loc['Bottom_Quartile'] = np.mean(bottom_quart_df[col])\n",
    "\n",
    "quartile_df[\"Difference\"] = quartile_df[\"Top_Quartile\"] - quartile_df[\"Bottom_Quartile\"]\n",
    "\n",
    "# negative Difference value indicates that it is bad for engagement\n",
    "# positive Difference value indicates that it is good for engagement\n",
    "quartile_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](quartile_table.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis and Advice\n",
    "\n",
    "For REI, based on our quartile difference analysis we have found that engagement on posts is increased when the picture\n",
    "is of a landscape. Post engagement is negatively affected when the caption talks about hiking gear and promotions and\n",
    "they are much worse for engagement if REI posts them as an image. Customer engagement improves a lot when REI posts\n",
    "about landscapes and challenges. This can be explained by analysing the platform of Instagram. Instagram users react more\n",
    "positively to lifestyle posts than direct ads. From our findings it would be better for post engagement if REI posted\n",
    "more about landscapes and challenges. Even though REI is a brand for gear and reseller for other brands, on instagram\n",
    "REI should try to establish a lifestyle improvement presence. REI should position their pictures and captions to relate\n",
    "to an active lifestyle. When making posts that feature promotions, REI must meld the promotion with their active lifestyle image in order to keep engagement with promotional posts high.\n",
    "\n",
    "This analysis comes from our findings of high quartile difference between top quartile and bottom quartile of topic\n",
    "analysis. We found that there was a high positive difference between top quartile and bottom quartile for landscape\n",
    "picture posts and challenges in captions. We also found a strong negative difference for promotions in both pictures and\n",
    "captions. We would also recommend that REI move text to the captions and take away all text from images.\n",
    "\n",
    "Based on the logistic regression we performed we found that predicting customer engagement on captions is more accurate\n",
    "than predicting customer engagement on attributes retrieved about images from the Google Vision API. We got a 65%\n",
    "accuracy using attributes from the Google vision API vs a 70% accuracy via using just the captions. Combining the two we\n",
    "got an accuracy of 68%. We found that the Google image description alone was not very useful in predicting engagement,\n",
    "but rather it's value was found in the use of topic modeling where the correlation between image and engagement became\n",
    "apparent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34d5b386c16fb96373a753b9697e9ce8f51af43b76cd80b9910f21c84deb8d01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
